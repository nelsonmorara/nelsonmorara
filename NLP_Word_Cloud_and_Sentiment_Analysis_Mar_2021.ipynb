{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Word_Cloud_and_Sentiment_Analysis_Mar_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelsonmorara/nelsonmorara/blob/main/NLP_Word_Cloud_and_Sentiment_Analysis_Mar_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtY8MnDXLlOM"
      },
      "source": [
        "reviews_file = open('/content/Dallas house yelp reviews.txt') \n",
        "text = reviews_file.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWofGv6V4p63"
      },
      "source": [
        "# Displaying the content of the reviews file\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX44Qg4-4uym"
      },
      "source": [
        "#Import required NLP libraries\n",
        "import nltk \n",
        "nltk.download('punkt') \n",
        "\n",
        "from nltk import sent_tokenize # this helps to split text into Sentences\n",
        "from nltk import word_tokenize # this helps to split text into individual Words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E45e-8Po4xbO"
      },
      "source": [
        "#Tokenize the text by sentences :\n",
        "sentences = sent_tokenize(text)\n",
        "#How many sentences are there? :\n",
        "print (len(sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUs4fEOU47VO"
      },
      "source": [
        "#Print the sentences :\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-I7cwcZ49Jd"
      },
      "source": [
        "#Tokenize the text with words :\n",
        "words = word_tokenize(text)\n",
        "\n",
        "#How many words are there? :\n",
        "print (len(words))\n",
        "print(\"\\n\")\n",
        "\n",
        "#Print words :\n",
        "print (words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-lGXC-Q4_wh"
      },
      "source": [
        "# Exploratory Analysis on the Text\n",
        "\n",
        "# Import required libraries\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Find the frequency of the words\n",
        "fdist = FreqDist(words)\n",
        "# Printing the frequency\n",
        "print(fdist)\n",
        "\n",
        "# Find the 10 most common words \n",
        "fdist_10 = fdist.most_common(10)\n",
        "print(fdist_10)\n",
        "\n",
        "# Plot the graph for fdist :\n",
        "import matplotlib.pyplot as plt\n",
        "fdist.plot(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqTpLqzo5DOO"
      },
      "source": [
        "# Text Data Cleaning - taking out the punctuations\n",
        "\n",
        "# Empty list to store words:\n",
        "words_no_punc = []\n",
        "\n",
        "#Removing punctuation marks :\n",
        "for w in words:\n",
        "    if w.isalpha():\n",
        "        words_no_punc.append(w.lower())\n",
        "\n",
        "#Print the words without punctuation marks :\n",
        "print (words_no_punc)\n",
        "print (\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2QiQBU9RBrk"
      },
      "source": [
        "#Length :\n",
        "print (len(words_no_punc))\n",
        "\n",
        "#Frequency distribution :\n",
        "fdist = FreqDist(words_no_punc)\n",
        "\n",
        "# Displaying the top 10 common words\n",
        "fdist.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFXy36qURFoe"
      },
      "source": [
        "#Plot the most common words on graph:\n",
        "fdist.plot(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsW0GsEy5IWv"
      },
      "source": [
        "# Text Data Cleaning loading the stopwords library\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#List of stopwords\n",
        "stopwords = stopwords.words(\"english\")\n",
        "print(stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-78_BjmPbn2"
      },
      "source": [
        "# Text Data Cleaning removing the stopwords from the text\n",
        "\n",
        "#Empty list to store clean words :\n",
        "clean_words = []\n",
        "\n",
        "for w in words_no_punc:\n",
        "    if w not in stopwords:\n",
        "        clean_words.append(w)\n",
        "        \n",
        "print(clean_words)\n",
        "print(\"\\n\")\n",
        "print(len(clean_words))\n",
        "\n",
        "#Frequency distribution :\n",
        "fdist = FreqDist(clean_words)\n",
        "fdist.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqnn57c8PQBN"
      },
      "source": [
        "#Plot the most common words on grpah:\n",
        "fdist.plot(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atTQ7dXY5Ms1"
      },
      "source": [
        "#Library to form wordcloud :\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "#Library to plot the wordcloud :\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Generating the wordcloud :\n",
        "wordcloud = WordCloud().generate(text)\n",
        "#Plot the wordcloud :\n",
        "plt.figure(figsize = (12, 12)) \n",
        "plt.imshow(wordcloud) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViARhYkh7QSF"
      },
      "source": [
        "# Sentiment Analysis\n",
        "# First, we import the relevant modules from the NLTK library\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv1NnCmb7Txd"
      },
      "source": [
        "# next, we initialize VADER so we can use it within our Python script\n",
        "sid = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEZ5V8jf7VgL"
      },
      "source": [
        "# Calling the polarity_scores method on sid and passing in the message_text outputs a dictionary \n",
        "# with negative, neutral, positive, and compound scores for the input text\n",
        "scores = sid.polarity_scores(text)\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKYXLw0G7hNR"
      },
      "source": [
        "# Below is the sentiment analysis code rewritten for sentence-level analysis\n",
        "# note the new module -- word_tokenize!\n",
        "import nltk.data\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk import sentiment\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU31Qao07oSl"
      },
      "source": [
        "# We will also initialize our 'english.pickle' function and give it a short name\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "# The tokenize method breaks up the paragraph into a list of strings.\n",
        "sentences = tokenizer.tokenize(text)\n",
        "\n",
        "# We add the additional step of iterating through the list of sentences and calculating and \n",
        "#printing polarity scores for each one.\n",
        "\n",
        "for sentence in sentences:\n",
        "        print(sentence)\n",
        "        scores = sid.polarity_scores(sentence)\n",
        "        for key in sorted(scores):\n",
        "                print('{0}: {1}, '.format(key, scores[key]), end='')\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmyODSrxexTK"
      },
      "source": [
        "# Testing with a sample script\n",
        "text = \"This product is good\"\n",
        "sentences = tokenizer.tokenize(text)\n",
        "for sentence in sentences:\n",
        "    scores = sid.polarity_scores(sentence)\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}